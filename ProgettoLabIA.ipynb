{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoLabIA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p9vej9xTCko7",
        "DhG79IiJQqyM"
      ],
      "authorship_tag": "ABX9TyPlJ8jP5jIcKSjiW7e3e+Be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElenaBianchini/ColoringGrayscaleImages/blob/main/ProgettoLabIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Progetto di Laboratorio di Intelligenza Artificiale e Grafica Interattiva**"
      ],
      "metadata": {
        "id": "p9vej9xTCko7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "3UMQe_W5sIUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "YvN1ozJhtapC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dwjW6FAknFw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6ajn5CNzCdX8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from skimage import color\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impostazione dei parametri"
      ],
      "metadata": {
        "id": "iC8ms4b01Gaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "XYgrYPlC1KfA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_gpu:\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "xZU13MrFxlbX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RALMpfJNgJx4"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ufJf6XFNgJx5"
      },
      "outputs": [],
      "source": [
        "root_path = \"/content/drive/MyDrive/COCO\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JvQXNYp-gJx5"
      },
      "outputs": [],
      "source": [
        "train_folder = root_path+\"/train2014/\"\n",
        "val_folder = root_path+\"/val2014/\"\n",
        "test_folder = root_path+\"/test2014/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "uJ0qK-Zou8HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadImagesName(dir_path):\n",
        "    images_list = []\n",
        "    count = 1\n",
        "    for image_name in os.listdir(dir_path):\n",
        "      filename = os.path.join(dir_path, image_name)\n",
        "      images_list.append(filename)\n",
        "      print(\"\\rImage num: {}\".format(count), end='')\n",
        "      count = count+1\n",
        "    \n",
        "    return images_list\n"
      ],
      "metadata": {
        "id": "DfM79xix_wfw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = loadImagesName(train_folder)"
      ],
      "metadata": {
        "id": "EZWOwGE4AOpM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "ea6cbdf5-446d-4f94-af23-792800e427b5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-4d9539600fe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadImagesName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-48e8801e7209>\u001b[0m in \u001b[0;36mloadImagesName\u001b[0;34m(dir_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/MyDrive/COCO/train2014/'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_list = loadImagesName(val_folder)"
      ],
      "metadata": {
        "id": "M5sAmirSDvoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = loadImagesName(test_folder)"
      ],
      "metadata": {
        "id": "HWM8f4cQDwgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images_list):\n",
        "    self.images_list = images_list\n",
        "    self.img_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.Lambda(self.import_image),\n",
        "      transforms.ToTensor(),                                \n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # RESNET si aspetta immagini preprocessate cos√¨\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images_list)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(self.images_list[idx]).convert('RGB')\n",
        "    img = self.img_transform(img)\n",
        "    return img\n",
        "\n",
        "  # converto ogni immagine in un tensore pytorch contenente un'immagine LAB\n",
        "  def import_image(self, img):\n",
        "    return color.rgb2lab(img) \n",
        "\n"
      ],
      "metadata": {
        "id": "wtmuB14T92XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(train_list)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "XNmmunfM0ZwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = ImageDataset(val_list)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "g6j214fXEGIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ImageDataset(test_list)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "-cOPIohWEIOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, data in enumerate(test_dataloader):\n",
        "  print(data.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "QOkSPvAnETBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione"
      ],
      "metadata": {
        "id": "8Rt-a1rL1oFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello"
      ],
      "metadata": {
        "id": "msVtIgsBPxes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationRNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ColorizationRNet, self).__init__()\n",
        "\n",
        "    # Importo ResNet che user√≤ per estrarre le features dalle immagini\n",
        "    resnet = torchvision.models.resnet18()\n",
        "    # Cambio il primo livello di convoluzione di ResNet per accetta input con un solo canale\n",
        "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
        "    # Estraggo le feature dalle immagini\n",
        "    self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
        "\n",
        "    # Livelli di deconvoluzione:\n",
        "    self.deconv = nn.Sequential(\n",
        "        nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Upsample(scale_factor=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.midlevel_resnet(x)\n",
        "    output = self.deconv(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "KY9qwmY51sh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_net = ColorizationRNet()"
      ],
      "metadata": {
        "id": "QwGrjI6SJ8eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzione di costo e di ottimizzazione"
      ],
      "metadata": {
        "id": "chVUarIbQOF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=reg_net.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "3y7KBtPDQdvW"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "ZXKFFqiKnH5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, loss_avg):\n",
        "  # Set model to training model\n",
        "  reg_net.train()\n",
        "\n",
        "  print('Starting training epoch {}'.format(epoch))\n",
        "\n",
        "  loss_avg.append(0)\n",
        "\n",
        "  for batch_idx, img in enumerate(train_dataloader):\n",
        "    img = img.to(device)\n",
        "\n",
        "    # Input alla rete: immagine in bianco e nero\n",
        "    img_grayscale = img[:, 0:1, :, :]\n",
        "\n",
        "    # Target: immagine ab \n",
        "    img_ab = img[:, 1:3, :, :]\n",
        "\n",
        "    # Predizione dell'immagine ab da grayscale\n",
        "    predicted = reg_net(img_grayscale)\n",
        "\n",
        "    # Calcolo l'errore L2 tra i colori ottenuti e quelli veri:\n",
        "    loss = criterion(predicted, img_ab)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Aggiorno i pesi:\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_avg[-1]+=loss.item()\n",
        "\n",
        "    if batch_idx % 12 == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx, len(train_dataloader), 100. * batch_idx / len(train_dataloader), loss.item()))\n",
        "\n",
        "  loss_avg[-1]/= batch_size\n",
        "  print('Finished training epoch {}'.format(epoch))\n"
      ],
      "metadata": {
        "id": "hdBRqhIdnK_t"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "SIQYIvBry5dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(epoch, val_loss_avg):\n",
        "  # Set model to validation model\n",
        "  reg_net.eval()\n",
        "\n",
        "  val_loss_avg.append(0)\n",
        "\n",
        "  for batch_idx, img in enumerate(val_dataloader):\n",
        "    img = img.to(device)\n",
        "\n",
        "    # Input alla rete: immagine in bianco e nero\n",
        "    img_grayscale = img[:, 0:1, :, :]\n",
        "\n",
        "    # Target: immagine ab \n",
        "    img_ab = img[:, 1:3, :, :]\n",
        "\n",
        "    # Predizione dell'immagine ab da grayscale\n",
        "    predicted = reg_net(img_grayscale)\n",
        "\n",
        "    # Calcolo l'errore L2 tra i colori ottenuti e quelli veri:\n",
        "    loss = criterion(predicted, img_ab)\n",
        "    val_loss_avg[-1]+=loss.item()\n",
        "  \n",
        "\n",
        "  val_loss_avg[-1]/= batch_size\n",
        "  print('\\nValidation set: Average loss: {:.4f}\\n'.format(val_loss_avg))\n",
        "  return val_loss_avg[-1]\n"
      ],
      "metadata": {
        "id": "aHHvNWsyy5dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allenamento"
      ],
      "metadata": {
        "id": "R3jCZTcQ-an9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_losses = 1.0\n",
        "train_loss_avg = []\n",
        "val_loss_avg = []\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  train(epoch, train_loss_avg)\n",
        "  losses = validation(epoch, val_loss_avg)\n",
        "\n",
        "  if losses < best_losses:\n",
        "    best_losses = losses\n",
        "    torch.save(reg_net.state_dict(), './checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))\n"
      ],
      "metadata": {
        "id": "-hy8YdTx-dop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grafico della curva di apprendimento"
      ],
      "metadata": {
        "id": "eOYzLm1znVAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ion()\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q-R9hLS5nbAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ion()\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "plt.plot(val_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HeZpTwP3CkAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Risultato su un'immagine del Test Set"
      ],
      "metadata": {
        "id": "AtEbe_4InkkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iUBuSBPmnzgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificazione "
      ],
      "metadata": {
        "id": "bwXYJnOS1sy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello"
      ],
      "metadata": {
        "id": "OmHFwyZEQqyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationCNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ColorizationCNet, self).__init__()\n",
        "\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Conv2d(1,64,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(64),\n",
        "\n",
        "        nn.Conv2d(64,128,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128,128,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128),\n",
        "\n",
        "        nn.Conv2d(128,256,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256,256,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(256),\n",
        "\n",
        "        nn.Conv2d(256,512,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.ConvTransposed2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256,256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(256, 313, kenel_size=1, stride=1, padding=0),\n",
        "        nn.Softmax(dim=1),\n",
        "        nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1),\n",
        "        nn.Upsample(scale_factor=4)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.network(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "eTNG7tehQqyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clas_net = ColorizationCNet()"
      ],
      "metadata": {
        "id": "K0GWb58aQqyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzione di costo e di ottimizzazione"
      ],
      "metadata": {
        "id": "DhG79IiJQqyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-lpIfiqMQqyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "KH5NjL1Zn31l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OqIVmVnYn31n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grafico della curva di apprendimento"
      ],
      "metadata": {
        "id": "DVHmWhFQn31o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GLryoGThn31p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valutazione sul Test Set"
      ],
      "metadata": {
        "id": "t1-5cz_Bn31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_UBigkc-n31q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Risultato su un'immagine del Test Set"
      ],
      "metadata": {
        "id": "vMWNrC0Tn31q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mmYgFIKnn31r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}