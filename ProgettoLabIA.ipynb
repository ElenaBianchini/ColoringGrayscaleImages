{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoLabIA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p9vej9xTCko7",
        "bwXYJnOS1sy6"
      ],
      "authorship_tag": "ABX9TyNS0lFadItEiS03Qt35U3o3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElenaBianchini/ColoringGrayscaleImages/blob/main/ProgettoLabIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Progetto di Laboratorio di Intelligenza Artificiale e Grafica Interattiva**"
      ],
      "metadata": {
        "id": "p9vej9xTCko7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "3UMQe_W5sIUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "YvN1ozJhtapC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwjW6FAknFw_",
        "outputId": "47f4232c-1b56-482b-f605-7face457cf45"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6ajn5CNzCdX8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from skimage import color\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impostazione dei parametri"
      ],
      "metadata": {
        "id": "iC8ms4b01Gaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "XYgrYPlC1KfA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_gpu:\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "xZU13MrFxlbX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "Lf6pZdGmFuXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528a6bbc-ed47-4be3-ff78-32faf4532c30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RALMpfJNgJx4"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ufJf6XFNgJx5"
      },
      "outputs": [],
      "source": [
        "root_path = \"/content/drive/MyDrive/COCO\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JvQXNYp-gJx5"
      },
      "outputs": [],
      "source": [
        "train_folder = root_path+\"/train2014/\"\n",
        "val_folder = root_path+\"/val2014/\"\n",
        "test_folder = root_path+\"/test2014/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "uJ0qK-Zou8HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadImagesName(dir_path, num):\n",
        "    images_list = []\n",
        "    count = 1\n",
        "    for image_name in os.listdir(dir_path):\n",
        "      if (count>num):\n",
        "        break\n",
        "      filename = os.path.join(dir_path, image_name)\n",
        "      images_list.append(filename)\n",
        "      print(\"\\rImage num: {}\".format(count), end='')\n",
        "      count = count+1\n",
        "    \n",
        "    return images_list\n"
      ],
      "metadata": {
        "id": "DfM79xix_wfw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = loadImagesName(train_folder, 10000)"
      ],
      "metadata": {
        "id": "EZWOwGE4AOpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e890462-d139-4503-c0c0-3e657c95956f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image num: 10000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_list = loadImagesName(val_folder, 5000)"
      ],
      "metadata": {
        "id": "M5sAmirSDvoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5185fd0-7e2f-404f-d0ca-ffc8edf6d378"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image num: 5000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = loadImagesName(test_folder, 3750)"
      ],
      "metadata": {
        "id": "HWM8f4cQDwgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6360d2a-6712-4405-afb2-375b73d0262a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image num: 3750"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images_list):\n",
        "    self.images_list = images_list\n",
        "    self.img_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      #transforms.Lambda(self.import_image),\n",
        "      #transforms.ToTensor(),                                \n",
        "      #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # RESNET si aspetta immagini preprocessate così\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images_list)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(self.images_list[idx]).convert('RGB')\n",
        "    img = self.img_transform(img)\n",
        "    img = np.asarray(img)\n",
        "    img_lab = color.rgb2lab(img)\n",
        "    img_lab = (img_lab + 128) / 255    # perché i valori dei canali ab del formato Lab vanno da -128 a 127 \n",
        "    img_ab = img_lab[:, :, 1:3]\n",
        "    img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "    img_gray = color.rgb2gray(img)\n",
        "    img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
        "    img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
        "    return img, img_ab, img_gray\n",
        "\n",
        "  # converto ogni immagine in un tensore pytorch contenente un'immagine LAB\n",
        "  def import_image(self, img):\n",
        "    return color.rgb2lab(img) \n",
        "\n"
      ],
      "metadata": {
        "id": "wtmuB14T92XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(train_list)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "XNmmunfM0ZwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = ImageDataset(val_list)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "g6j214fXEGIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ImageDataset(test_list)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "-cOPIohWEIOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione"
      ],
      "metadata": {
        "id": "8Rt-a1rL1oFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello"
      ],
      "metadata": {
        "id": "msVtIgsBPxes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationRNet(nn.Module):\n",
        "  def __init__(self, input_size = 128):\n",
        "    super(ColorizationRNet, self).__init__()\n",
        "\n",
        "    # Importo ResNet che userò per estrarre le features dalle immagini\n",
        "    resnet = torchvision.models.resnet18()\n",
        "    # Cambio il primo livello di convoluzione di ResNet per accetta input con un solo canale\n",
        "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
        "    # Estraggo le feature dalle immagini\n",
        "    self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
        "\n",
        "    # Livelli di deconvoluzione:\n",
        "    self.deconv = nn.Sequential(\n",
        "        nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Upsample(scale_factor=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.midlevel_resnet(x)\n",
        "    output = self.deconv(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "KY9qwmY51sh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_net = ColorizationRNet()\n",
        "reg_net = reg_net.to(device)"
      ],
      "metadata": {
        "id": "QwGrjI6SJ8eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzione di costo e di ottimizzazione"
      ],
      "metadata": {
        "id": "chVUarIbQOF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=reg_net.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "3y7KBtPDQdvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "ZXKFFqiKnH5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, loss_avg):\n",
        "  # Set model to training model\n",
        "  reg_net.train()\n",
        "\n",
        "  print('\\nStarting training epoch {}\\n'.format(epoch))\n",
        "\n",
        "  loss_avg.append(0)\n",
        "\n",
        "  for batch_idx, (img, img_ab, img_gray) in enumerate(train_dataloader):\n",
        "    img = img.to(device)\n",
        "    img_ab = img_ab.to(device)\n",
        "    img_gray = img_gray.to(device)\n",
        "\n",
        "    # Predizione dell'immagine ab da grayscale\n",
        "    predicted = reg_net(img_gray)\n",
        "\n",
        "    # Calcolo l'errore L2 tra i colori ottenuti e quelli veri:\n",
        "    loss = criterion(predicted, img_ab)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Aggiorno i pesi:\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_avg[-1]+=loss.item()\n",
        "\n",
        "    if batch_idx % 12 == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx*len(img), len(train_dataloader.dataset), 100. * batch_idx / len(train_dataloader), loss.item()))\n",
        "\n",
        "  loss_avg[-1]/= batch_size\n",
        "  print('\\nFinished training epoch {}\\n'.format(epoch))\n"
      ],
      "metadata": {
        "id": "hdBRqhIdnK_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "SIQYIvBry5dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(epoch, val_loss_avg):\n",
        "  # Set model to validation model\n",
        "  reg_net.eval()\n",
        "\n",
        "  val_loss_avg.append(0)\n",
        "\n",
        "  for batch_idx, (img, img_ab, img_gray)in enumerate(val_dataloader):\n",
        "    img = img.to(device)\n",
        "    img_ab = img_ab.to(device)\n",
        "    img_gray = img_gray.to(device)\n",
        "\n",
        "    # Predizione dell'immagine ab da grayscale\n",
        "    predicted = reg_net(img_gray)\n",
        "\n",
        "    # Calcolo l'errore L2 tra i colori ottenuti e quelli veri:\n",
        "    loss = criterion(predicted, img_ab)\n",
        "    val_loss_avg[-1]+=loss.item()\n",
        "  \n",
        "\n",
        "  val_loss_avg[-1]/= batch_size\n",
        "  print('\\nValidation set: Average loss: {:.4f}\\n'.format(val_loss_avg))\n",
        "  return val_loss_avg[-1]\n"
      ],
      "metadata": {
        "id": "aHHvNWsyy5dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allenamento"
      ],
      "metadata": {
        "id": "R3jCZTcQ-an9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_losses = 1.0\n",
        "train_loss_avg = []\n",
        "val_loss_avg = []\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  train(epoch, train_loss_avg)\n",
        "  losses = validation(epoch, val_loss_avg)\n",
        "\n",
        "  if losses < best_losses:\n",
        "    best_losses = losses\n",
        "    torch.save(reg_net.state_dict(), '/content/drive/MyDrive/ProgettoLab/checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))\n"
      ],
      "metadata": {
        "id": "-hy8YdTx-dop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grafico della curva di apprendimento"
      ],
      "metadata": {
        "id": "eOYzLm1znVAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ion()\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q-R9hLS5nbAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ion()\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "plt.plot(val_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HeZpTwP3CkAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Risultato su un'immagine del Test Set"
      ],
      "metadata": {
        "id": "AtEbe_4InkkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iUBuSBPmnzgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificazione "
      ],
      "metadata": {
        "id": "bwXYJnOS1sy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello"
      ],
      "metadata": {
        "id": "OmHFwyZEQqyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationCNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ColorizationCNet, self).__init__()\n",
        "\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Conv2d(1,64,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(64),\n",
        "\n",
        "        nn.Conv2d(64,128,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128,128,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128),\n",
        "\n",
        "        nn.Conv2d(128,256,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256,256,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(256),\n",
        "\n",
        "        nn.Conv2d(256,512,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=2, dilatation=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "\n",
        "        nn.ConvTransposed2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256,256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(256, 313, kenel_size=1, stride=1, padding=0),\n",
        "        nn.Softmax(dim=1),\n",
        "        nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1),\n",
        "        nn.Upsample(scale_factor=4)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.network(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "eTNG7tehQqyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clas_net = ColorizationCNet()"
      ],
      "metadata": {
        "id": "K0GWb58aQqyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzione di costo e di ottimizzazione"
      ],
      "metadata": {
        "id": "DhG79IiJQqyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-lpIfiqMQqyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "KH5NjL1Zn31l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OqIVmVnYn31n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grafico della curva di apprendimento"
      ],
      "metadata": {
        "id": "DVHmWhFQn31o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GLryoGThn31p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valutazione sul Test Set"
      ],
      "metadata": {
        "id": "t1-5cz_Bn31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_UBigkc-n31q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Risultato su un'immagine del Test Set"
      ],
      "metadata": {
        "id": "vMWNrC0Tn31q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mmYgFIKnn31r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}